\documentclass[12pt]{article}
\usepackage{amsmath}

% page setup
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.0in}
\sloppy\sloppypar\raggedbottom\frenchspacing

% math definitions
\newcommand{\given}{\,|\,}
\newcommand{\T}{^{{\mathsf T}}}
\newcommand{\inv}{^{-1}}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\noindent
Dear Bernhard,

I am writing this to summarize some realizations or ideas that came up
at the Ringberg meeting on Machine Learning and Astronomy. I'm writing
to ask if the following conjecture is reasonable? I'd love to discuss
in person or by phone if you are interested.

Hogg

\paragraph{executive summary:}
A sandbox is created in which both generative and discriminative
regressions are trained on toy training data, and tested on toy test
data.
The regressions are linear and all of the toy data (or at least up to
additive noise) were truly generated with a linear model that lives in
the space available to the linear regressions.
My conjecture (developed with Soledad Villar at NYU) is that for all
training-set sizes, for all (reasonable) settings of regularization parameters,
the generative-direction regressions produce more accurate inferences on
test data than the discriminative-direction regressions.
Empirical experiments support the conjecture, but it is \emph{so simple}
that it feels like it must be either wrong or already known.

\paragraph{problem set-up:}
In what follows the training set will contain $N$ data $x_n$, each of
which is a $D$-vector.
Each training datum $x_n$ has an associated label $y_n$, which is a
$K$-vector.

Known to God, these training data were generated by a linear process
which can be expressed as
\begin{eqnarray}
x_n &=& Q \cdot y_n + \epsilon_n \\
p(y_n) &=& N(y_n\given 0, 1) \\
p(\epsilon_n) &=& N(\epsilon_n\given 0, C)
~,
\end{eqnarray}
where $Q$ is God's universal $D\times K$ generative matrix, and
$\epsilon_n$ is a normal deviate (noise) drawn with mean zero and
$D\times D$ variance tensor $C$.

Given these training data, investigators might train regressions to
predict the label $y^\ast$ for some new test datum $x^\ast$.
These investigators have choices.
One is whether to train a \emph{generative} model or a
\emph{discriminative} model.

Exceedingly briefly, here is the generative regression:
\begin{eqnarray}
A &\leftarrow& \argmin_{A'} \sum_{n=1}^N ||x_n - A'\cdot y_n|| \\
A^\dagger &\equiv& [A\T\cdot A]\inv \cdot A\T \\
y^\ast &=& A^\dagger\cdot x^\ast
\end{eqnarray}

\end{document}
