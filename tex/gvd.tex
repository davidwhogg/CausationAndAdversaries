\documentclass[12pt]{article}
\usepackage{amsmath}

% page setup
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.0in}
\sloppy\sloppypar\raggedbottom\frenchspacing

% math definitions
\newcommand{\given}{\,|\,}
\newcommand{\T}{^{{\mathsf T}}}
\newcommand{\inv}{^{-1}}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\noindent
Dear Bernhard,

\emph{I am writing this to summarize some realizations or ideas that came up
at the Ringberg meeting on Machine Learning and Astronomy. I'm writing
to ask if the following conjecture is reasonable? I'd love to discuss
in person or by phone if you are interested.}

Hogg

\paragraph{executive summary:}
A sandbox is created in which both generative and discriminative
regressions are trained on toy training data, and tested on toy test
data.
The regressions are linear and all of the toy data (or at least up to
additive noise) were truly generated with a linear model that lives in
the space available to the linear regressions.
My conjecture (developed with Soledad Villar at NYU) is that for all
training-set sizes, for all (reasonable) settings of regularization parameters,
the generative-direction regressions produce more accurate inferences on
test data than the discriminative-direction regressions.
Empirical experiments support the conjecture, but it is \emph{so simple}
that it feels like it must be either wrong or already known.

\paragraph{problem set-up:}
In what follows the training set will contain $N$ data $x_n$, each of
which is a $D$-vector.
Each training datum $x_n$ has an associated label $y_n$, which is a
$K$-vector.

Known to God, these training data were generated by a linear process
which can be expressed as
\begin{align}
x_n &= Q \cdot y_n + \epsilon_n \\
p(y_n) &= N(y_n\given 0, 1) \\
p(\epsilon_n) &= N(\epsilon_n\given 0, C_n)
~,
\end{align}
where $Q$ is God's universal $D\times K$ generative matrix, and
$\epsilon_n$ is a normal deviate (noise) drawn with mean zero and
$D\times D$ variance tensor $C_n$.
We will additionally assume (although everything can be generalized
relatively straightforwardly) that all the $C_n$ are identical,
diagonal, and proportional to the identity.

Given these training data, investigators might train regressions to
predict the label $y^\ast$ for some new test datum $x^\ast$.
These investigators have choices.
One is whether to train a \emph{generative} model or a
\emph{discriminative} model.
Exceedingly briefly, here is the generative regression:
\begin{align}
A &\leftarrow \argmin_{A'} \sum_{n=1}^N ||x_n - A'\cdot y_n||_2^2 \\
A^\dagger &\equiv [A\T\cdot A]\inv \cdot A\T \\
y^\ast &\leftarrow A^\dagger\cdot x^\ast
~,
\end{align}
where $A$ is a $D\times K$ matrix and $A^\dagger$ is its $K\times D$
pseudo-inverse.
Here is the discriminative regression:
\begin{align}
B &\leftarrow \argmin_{B'} \sum_{n=1}^N ||y_n - B'\cdot x_n||_2^2 + \lambda\,||B||_2^2 \\
y^\ast &\leftarrow B\cdot x^\ast
~,
\end{align}
where $B$ is a $K\times D$ matrix and there is a regularization term
because we are going to need it in what follows.

\paragraph{information theory:}
An investigator with access to God's universal matrix $Q$ would infer
the new label $y^\ast$ with the pseudo-inverse of $Q$.
\begin{align}
Q^\dagger &\equiv [Q\T\cdot Q]\inv \cdot Q\T \\
y^\ast &\leftarrow Q^\dagger\cdot x^\ast
~.
\end{align}
This estimator for $y^\ast$, based on the pseudo-inverse of $Q$
is---under our assumptions---the asymptotically unbiased efficient
estimator for the label $y^\ast$. \emph{Or something like that!}
This means that any method for estimating $y^\ast$ approaches
information-theory bounds as it approaches a left-multiply by
$Q^\dagger$.

\paragraph{conjecture:}


\paragraph{comments:}


\end{document}
