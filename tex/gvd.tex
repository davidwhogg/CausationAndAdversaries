% This file is part of the CausationAndAdversaries project.
% Copyright 2019 David W Hogg (NYU) and Soledad Villar (NYU).

% style notes
% -----------
% - always align, never equation or eqnarray

\documentclass[12pt]{article}
\usepackage{amsmath}

% page setup
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.0in}
\sloppy\sloppypar\raggedbottom\frenchspacing

% math definitions
\newcommand{\given}{\,|\,}
\newcommand{\T}{^{{\mathsf T}}}
\newcommand{\inv}{^{-1}}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\noindent
{\slshape Dear Bernhard,

I am writing this to summarize some realizations or ideas that came up
at the Ringberg meeting on Machine Learning and Astronomy. I'm writing
to ask if the following conjecture is reasonable? I'd love to discuss
in person or by phone if you are interested.

~\hfill Hogg}

\paragraph{executive summary:}
A sandbox is created in which both generative and discriminative
regressions are trained on toy training data, and tested on toy test
data.
The regressions are linear and all of the toy data (or at least up to
additive noise) were truly generated with a linear model that lives in
the space available to the linear regressions.
My conjecture (developed with Soledad Villar at NYU) is that for all
training-set sizes, for all (reasonable) settings of regularization parameters,
the generative-direction regressions produce more accurate inferences on
test data than the discriminative-direction regressions.
Empirical experiments support the conjecture, but it is \emph{so simple}
that it feels like it must be either wrong or already known.

\paragraph{problem set-up:}
In what follows the training set will contain $N$ data $x_n$, each of
which is a $D$-vector.
Each training datum $x_n$ has an associated label $y_n$, which is a
$K$-vector.
We further assume that the data-space dimensionality $D$ is much larger
than the label-space dimensionality $K$ (many more pixels than labels).

Known to God, these training data were generated by a linear process
which can be expressed as
\begin{align}
x_n &= Q \cdot y_n + \epsilon_n \\
p(y_n) &= N(y_n\given 0, 1) \\
p(\epsilon_n) &= N(\epsilon_n\given 0, C_n)
~,
\end{align}
where $Q$ is God's universal $D\times K$ generative matrix, and
$\epsilon_n$ is a normal deviate (noise) drawn with mean zero and
$D\times D$ variance tensor $C_n$.
We will additionally assume (although everything can be generalized
relatively straightforwardly) that all the $C_n$ are identical, equal
to some common tensor $C$, diagonal, and proportional to the identity:
\begin{align}
C_n &= C = \sigma^2\,I_D
~.
\end{align}

Given these training data, investigators might train regressions to
predict the label $y^\ast$ for some new test datum $x^\ast$.
These investigators have choices.
One is whether to train a \emph{generative} model or a
\emph{discriminative} model.
Exceedingly briefly, here is the generative regression:
\begin{align}\label{eq:gen}
A &\leftarrow \argmin_{A'} \sum_{n=1}^N ||x_n - A'\cdot y_n||_2^2 \\
A^\dagger &\equiv [A\T\cdot A]\inv \cdot A\T \\
y^\ast &\leftarrow A^\dagger\cdot x^\ast
~,
\end{align}
where $A$ is a $D\times K$ matrix and $A^\dagger$ is its $K\times D$
pseudo-inverse.
Here is the discriminative regression:
\begin{align}\label{eq:dis}
B &\leftarrow \argmin_{B'} \sum_{n=1}^N ||y_n - B'\cdot x_n||_2^2 + \lambda\,||B||_2^2 \\
y^\ast &\leftarrow B\cdot x^\ast
~,
\end{align}
where $B$ is a $K\times D$ matrix and there is a regularization term
because we are going to need it in what follows.

\paragraph{information theory:}
An investigator with access to God's universal matrix $Q$ would infer
the new label $y^\ast$ with the pseudo-inverse of $Q$, to wit
\begin{align}
Q^\dagger &\equiv [Q\T\cdot Q]\inv \cdot Q\T \\
y^\ast &\leftarrow Q^\dagger\cdot x^\ast
~.
\end{align}
This estimator for $y^\ast$, based on the pseudo-inverse of $Q$
is---under our assumptions---the asymptotically unbiased efficient
estimator for the label $y^\ast$. \emph{Or something like that!}
This means that any method for estimating $y^\ast$ approaches
information-theory bounds as it approaches a left-multiply by
$Q^\dagger$.

\paragraph{conjecture:}
Our conjecture is that---under these assumptions---the generative
regression produces better (closer to information-theory-optimal)
predictions of labels for new data than the discriminative regression.
Further, our conjecture is that this will be true when $K<N<D$, and
also when $N > D$.
Further, our conjecture is that this will be true for any setting of
the discriminative regularization parameter $\lambda$.

This conjecture has been demonstrated (to some extent) in a
\textsl{Colaboratory}(tm) notebook which I can share with you.

\paragraph{comments:}
1.~When the noise is precisely zero ($C_n=C= 0$), the pseudo-inverse
$A^\dagger$ of any solution of the generative optimization
(\ref{eq:gen}) will also be a solution of the unregularized ($\lambda = 0$) discriminative
optimization (\ref{eq:dis}), and any solution $B$ of the discriminative
optimization (\ref{eq:dis}) will also be some kind of pseudo-inverse of a
solution $A$ of the generative optimization (\ref{eq:gen}) in the sense
that $B\cdot A=I_K$.
That is, in the zero-noise case, the discriminative and generative
methods will produce the same inferences.
Interestingly, we do not find this by taking the limit $C\rightarrow
0$; we find this by direct analysis of the zero-noise case.

2.~The conjecture is about an asymmetry. There are two different
asymmetries from which the conjectured asymmetry might flow.
One asymmetry is that the label dimensionality $K$ is much smaller
than the data dimensionality $D$.
Another asymmetry is that noise has been added to the data points
$x_n$, but not to the labels $y_n$.
If our conjecture flows from the noise asymmetry, then we can imagine
reversing this asymmetry and reversing the conjecture.
In this case we could add noise with $K\times K$ variance tensor $V$ to the labels $y_n$.
A secondary conjecture could be that the primary conjecture depends on whether
the following condition holds:
\begin{align}
\det(V\inv) > \det(Q^T\cdot C\inv\cdot Q)
~.
\end{align}
That is, the secondary conjecture is that the primary conjecture only holds
if the information (Fisher information) is larger in the labels than the data.
At present I have no evidence for this secondary conjecture.

\end{document}
